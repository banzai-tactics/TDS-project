{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "Installing requirements, auto reload changing to code, imports and some configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from helpers import utils, pipelines, models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from data_augmentaion.data_augmentator import DataAugmentor \n",
    "\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable tqdm progress bar by default\n",
    "from tqdm import tqdm\n",
    "from functools import partialmethod\n",
    "\n",
    "tqdm.__init__ = partialmethod(tqdm.__init__, disable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define research parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_task = False\n",
    "# continuous_features = []\n",
    "metric = 'accuracy'\n",
    "test_size_proportion = 0.2\n",
    "augment_sample = 0.5\n",
    "metrics = ['f1_weighted', 'accuracy', 'balanced_accuracy', 'precision_weighted', 'recall_weighted', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted'] \n",
    "\n",
    "search_pipelines = pipelines.get_classification_pipelines()\n",
    "search_parameters = models.parameters\n",
    "\n",
    "settings = [\n",
    "    {'method': 'random'},\n",
    "    {'method': 'smote'},\n",
    "    {'method': 'cf_random'},\n",
    "    # {'method': 'cf_genetic', 'kw_args': {'proximity_weight': 0.2, 'diversity_weight': 5, 'sparsity_weight': 0.2}},\n",
    "    # {'method': 'cf_genetic', 'kw_args': {'proximity_weight': 0.2, 'diversity_weight': 5, 'sparsity_weight': 1}},\n",
    "    {'method': 'cf_genetic', 'kw_args': {'proximity_weight': 5, 'diversity_weight': 0.2, 'sparsity_weight': 0.2}},\n",
    "    # {'method': 'cf_genetic', 'kw_args': {'proximity_weight': 5, 'diversity_weight': 0.2, 'sparsity_weight': 1}},\n",
    "    # {'method': 'cf_genetic', 'kw_args': {'proximity_weight': 1, 'diversity_weight': 1, 'sparsity_weight': 0.2}},\n",
    "    # {'method': 'cf_genetic', 'kw_args': {'proximity_weight': 1, 'diversity_weight': 1, 'sparsity_weight': 1}},\n",
    "    # {'method': 'cf_kdtree', 'kw_args': {'sparsity_weight': 0.2}},\n",
    "    # {'method': 'cf_kdtree', 'kw_args': {'sparsity_weight': 1}},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Define the parameters of the synthetic dataset\n",
    "n_samples = 600  # Total number of samples\n",
    "n_features = 12   # Number of features\n",
    "n_classes = 3    # Number of classes\n",
    "class_weights = [0.5, 0.2, 0.3]  # Class imbalance ratio\n",
    "\n",
    "# Create the imbalanced dataset\n",
    "X, y = make_classification(n_samples=n_samples, \n",
    "                           n_features=n_features,\n",
    "                           n_informative=3,\n",
    "                           n_classes=n_classes,\n",
    "                           n_clusters_per_class=2,\n",
    "                           weights=class_weights,\n",
    "                           class_sep=0.5, # 1\n",
    "                           random_state=42)\n",
    "X = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(n_features)])\n",
    "y = pd.Series(y, name=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_proportion, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole dataset scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'whole adult dataset {metric} scores:')\n",
    "whole_best_ests, whole_scores = utils.fit_and_evaluate(X_train, y_train, X_test, y_test,\n",
    "                    search_estimators=search_pipelines, search_params=search_parameters, scoring=metric)\n",
    "whole_dataset_result_df = pd.DataFrame.from_dict(whole_scores, orient='index')\n",
    "whole_dataset_result_df.columns = pd.MultiIndex.from_product([['whole']] + [whole_dataset_result_df.columns])\n",
    "whole_dataset_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results_df = pd.concat([full_results_df, whole_dataset_result_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimators = {}\n",
    "best_scores = {}\n",
    "total_time = time.time()\n",
    "for i, s in enumerate(settings):\n",
    "    start = time.time()\n",
    "    print(f'{i} / {len(settings)}, {s}', end=' ')\n",
    "    augmentor = DataAugmentor(X_train, y_train, X_test, y_test,\n",
    "                             method=s['method'], regression=regression_task,\n",
    "                             continuous_feats=continuous_features,\n",
    "                             cf_scoring = metric,\n",
    "                             kw_args=s.get('kw_args', {})\n",
    "                             )\n",
    "\n",
    "    X_train_augmented_balanced, y_train_augmented_balanced = augmentor.augment(balance=True)\n",
    "    best_estimators[f'{i}_balanced'], best_scores[f'{i}_balanced'] = \\\n",
    "     utils.fit_and_evaluate(X_train_augmented_balanced, y_train_augmented_balanced, X_test, y_test,\n",
    "                            search_estimators=search_pipelines, search_params=search_parameters, scoring=metrics)\n",
    "    result_df_balanced = pd.DataFrame.from_dict(best_scores[f'{i}_balanced'], orient='index')\n",
    "    result_df_balanced.columns = pd.MultiIndex.from_product([[f'{(list(s.values())[0])} balanced']] + [result_df_balanced.columns])\n",
    "    # result_df_balanced.columns = pd.MultiIndex.from_product([[f'{json.dumps((list(s.values())))} balanced']] + [result_df_balanced.columns])\n",
    "\n",
    "\n",
    "    X_train_augmented, y_train_augmented = augmentor.augment(balance=False, size=augment_sample)\n",
    "    best_estimators[f'{i}'], best_scores[f'{i}'] = \\\n",
    "        utils.fit_and_evaluate(X_train_augmented, y_train_augmented, X_test, y_test,\n",
    "                               search_estimators=search_pipelines, search_params=search_parameters, scoring=metrics)\n",
    "    result_df = pd.DataFrame.from_dict(best_scores[f'{i}'], orient='index')\n",
    "    result_df.columns = pd.MultiIndex.from_product([[f'{(list(s.values())[0])}']] + [result_df.columns])\n",
    "    # print(list(s.values())[0])\n",
    "\n",
    "    full_results_df = pd.concat([full_results_df, result_df], axis=1)\n",
    "    print(f'{time.time() - start} seconds for settings {i}')\n",
    "print(f'\\nTotal time: {time.time() - total_time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best methods:')\n",
    "display(utils.get_best_methods(full_results_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_results_as_latex_tables(full_results_df, task_name=\"artifical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_cols = ['whole', 'random','smote','cf_random','cf_genetic']\n",
    "metric_names = {'f1_weighted': 'F1',\n",
    "                'accuracy': 'Accuracy',\n",
    "                'balanced_accuracy': 'Balanced accuracy',\n",
    "                'precision_weighted': 'Precision',\n",
    "                'recall_weighted': 'Recall',\n",
    "                'roc_auc_ovr_weighted': 'ROC AUC OVR',\n",
    "                'roc_auc_ovo_weighted': 'ROC AUC OVO'}\n",
    "utils.spider_plot(full_results_df, 'lg', wanted_cols, metric_names, 'Logistic regression', save_task_name='artifical')\n",
    "utils.spider_plot(full_results_df, 'rf', wanted_cols, metric_names, 'Random Forest', save_task_name='artifical')\n",
    "utils.spider_plot(full_results_df, 'xgb', wanted_cols, metric_names, 'XGBoost', save_task_name='artifical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results_df.to_csv(rf'../log/experiment_multiclass_artifical.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
