{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "Installing requirements, auto reload changing to code, imports and some configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from helpers import utils, pipelines, models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from data_augmentaion.data_augmentator import DataAugmentor \n",
    "\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Dice library using tdqm to print progress bar.\n",
    "# For our use its very annoying because it print one for every generation,\n",
    "# and we genrating hundreds and sometimes thousands of counterfactuals.\n",
    "\n",
    "# disable tqdm progress bar by default\n",
    "from tqdm import tqdm\n",
    "from functools import partialmethod\n",
    "\n",
    "tqdm.__init__ = partialmethod(tqdm.__init__, disable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define research parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_task = False\n",
    "continuous_features = ['age', 'fnlwgt', 'hours-per-week']\n",
    "# metric = 'f1'\n",
    "metrics = ['f1', 'accuracy', 'precision', 'recall', 'roc_auc']\n",
    "test_size_proportion = 0.10\n",
    "augment_sample = 0.5\n",
    "sample_frac = 0.05\n",
    "\n",
    "search_pipelines = pipelines.get_classification_pipelines()\n",
    "search_parameters = models.parameters\n",
    "\n",
    "settings = [\n",
    "    {'method': 'random'},\n",
    "    {'method': 'smote'},\n",
    "    {'method': 'cf_random'},\n",
    "    # {'method': 'cf_genetic', 'kw_args': {'proximity_weight': 0.2, 'diversity_weight': 5, 'sparsity_weight': 0.2}},\n",
    "    # {'method': 'cf_genetic', 'kw_args': {'proximity_weight': 0.2, 'diversity_weight': 5, 'sparsity_weight': 1}},\n",
    "    {'method': 'cf_genetic', 'kw_args': {'proximity_weight': 7, 'diversity_weight': 0.2, 'sparsity_weight': 0.2}},\n",
    "    # {'method': 'cf_genetic', 'kw_args': {'proximity_weight': 5, 'diversity_weight': 0.2, 'sparsity_weight': 1}},\n",
    "    # {'method': 'cf_genetic', 'kw_args': {'proximity_weight': 1, 'diversity_weight': 1, 'sparsity_weight': 0.2}},\n",
    "    # {'method': 'cf_genetic', 'kw_args': {'proximity_weight': 1, 'diversity_weight': 1, 'sparsity_weight': 1}},\n",
    "#     {'method': 'cf_kdtree', 'kw_args': {'sparsity_weight': 0.2}},\n",
    "#     {'method': 'cf_kdtree', 'kw_args': {'sparsity_weight': 1}},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"../datasets/adult.csv\")\n",
    "df = utils.preprocess_adult(raw_data)\n",
    "target = 'income'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all data\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_proportion, random_state=42)\n",
    "\n",
    "# sampled data\n",
    "df_sample = df.sample(frac=sample_frac, random_state=42)\n",
    "X_sample = df_sample.drop(target, axis=1)\n",
    "y_sample = df_sample[target]\n",
    "X_sample_train, X_sample_test, y_sample_train, y_sample_test = train_test_split(X_sample, y_sample, test_size=test_size_proportion, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train target distribution:')\n",
    "display(y_train.value_counts())\n",
    "print('sampled train target distribution:')\n",
    "display(y_sample_train.value_counts())\n",
    "print(f'y_sample_train classes gap: {y_sample_train.value_counts().max()-y_sample_train.value_counts().min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole dataset scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'whole adult dataset scores:')\n",
    "_, whole_scores = utils.fit_and_evaluate(X_train, y_train, X_test, y_test,\n",
    "                    search_estimators=search_pipelines, search_params=search_parameters, scoring=metrics)\n",
    "whole_dataset_result_df = pd.DataFrame.from_dict(whole_scores, orient='index')\n",
    "whole_dataset_result_df.columns = pd.MultiIndex.from_product([['whole']] + [whole_dataset_result_df.columns])\n",
    "display(whole_dataset_result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampled adult dataset scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{sample_frac*100}% sampled adult dataset scores:')\n",
    "_, sampled_scores = utils.fit_and_evaluate(X_sample_train, y_sample_train, X_sample_test, y_sample_test,\n",
    "                                           search_estimators=search_pipelines, search_params=search_parameters, scoring=metrics)\n",
    "sampled_dataset_result_df = pd.DataFrame.from_dict(sampled_scores, orient='index')\n",
    "sampled_dataset_result_df.columns = pd.MultiIndex.from_product([['sample']] + [sampled_dataset_result_df.columns])\n",
    "sampled_dataset_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results_df = pd.concat([full_results_df, whole_dataset_result_df, sampled_dataset_result_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimators = {}\n",
    "best_scores = {}\n",
    "total_time = time.time()\n",
    "for i, s in enumerate(settings):\n",
    "    start = time.time()\n",
    "    print(f'{i+1} / {len(settings)}, {s}', end=' ')\n",
    "    augmentor = DataAugmentor(X_sample_train, y_sample_train, X_sample_test, y_sample_test,\n",
    "                             method=s['method'], regression=regression_task,\n",
    "                             continuous_feats=continuous_features,\n",
    "                             kw_args=s.get('kw_args', {})\n",
    "                             )\n",
    "\n",
    "    X_train_augmented_balanced, y_train_augmented_balanced = augmentor.augment(balance=True)\n",
    "    best_estimators[f'{i}_balanced'], best_scores[f'{i}_balanced'] = \\\n",
    "    utils.fit_and_evaluate(X_train_augmented_balanced, y_train_augmented_balanced, X_sample_test, y_sample_test,\n",
    "                            search_estimators=search_pipelines, search_params=search_parameters, scoring=metrics)\n",
    "    result_df_balanced = pd.DataFrame.from_dict(best_scores[f'{i}_balanced'], orient='index')\n",
    "    result_df_balanced.columns = pd.MultiIndex.from_product([[f'{(list(s.values())[0])} balanced']] + [result_df_balanced.columns])\n",
    "    # result_df_balanced.columns = pd.MultiIndex.from_product([[f'{json.dumps((list(s.values())))} balanced']] + [result_df_balanced.columns])\n",
    "\n",
    "\n",
    "    X_train_augmented, y_train_augmented = augmentor.augment(balance=False, size=augment_sample)\n",
    "    best_estimators[f'{i}'], best_scores[f'{i}'] = \\\n",
    "        utils.fit_and_evaluate(X_train_augmented, y_train_augmented, X_sample_test, y_sample_test,\n",
    "                               search_estimators=search_pipelines, search_params=search_parameters, scoring=metrics)\n",
    "    result_df = pd.DataFrame.from_dict(best_scores[f'{i}'], orient='index')\n",
    "    result_df.columns = pd.MultiIndex.from_product([[f'{(list(s.values())[0])}']] + [result_df.columns])\n",
    "    # result_df.columns = pd.MultiIndex.from_product([[f'{json.dumps((list(s.values())))}']] + [result_df.columns])\n",
    "\n",
    "    full_results_df = pd.concat([full_results_df, result_df_balanced, result_df], axis=1)\n",
    "    print(f'{time.time() - start} seconds for settings {i}')\n",
    "\n",
    "print(f'\\nTotal time: {time.time() - total_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best methods:')\n",
    "display(utils.get_best_methods(full_results_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_results_as_latex_tables(full_results_df, task_name=\"adult\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_cols = ['whole', 'sample', 'random', 'smote', 'cf_random', 'cf_genetic']\n",
    "metric_names = {'f1': 'F1',\n",
    "                'accuracy': 'Accuracy',\n",
    "                'precision': 'Precision',\n",
    "                'recall': 'Recall',\n",
    "                'roc_auc': 'ROC AUC'}\n",
    "utils.spider_plot(full_results_df, 'lg', wanted_cols, metric_names, 'Logistic regression', save_task_name='adult')\n",
    "utils.spider_plot(full_results_df, 'rf', wanted_cols, metric_names, 'Random Forest', save_task_name='adult')\n",
    "utils.spider_plot(full_results_df, 'xgb', wanted_cols, metric_names, 'XGBoost', save_task_name='adult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results_df.to_csv(rf'../log/experiment_classification_adult_testsize{test_size_proportion}_augmentsample{augment_sample}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
